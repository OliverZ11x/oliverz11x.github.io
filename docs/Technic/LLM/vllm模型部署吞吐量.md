---
date created: 2025/6/26 9:37
date modified: 2025/6/27 10:56
---

要估算在给定硬件上使用 Docker 和 vLLM 部署 Qwen3-1.7b 的理论吞吐量，需要综合考虑硬件配置、模型特性和 vLLM 的优化能力。以下是详细分析：

### 一、硬件配置分析

#### 1. CPU 配置

- **型号**：Intel Xeon Platinum 8457C
- **核心数**：22 核/插座，支持超线程（每核2线程），总线程数 **44**
- **特性**：适合处理并发请求和预处理任务，但推理核心在 GPU

#### 2. GPU 配置

- **型号**：2 × NVIDIA L20
- **显存**：每卡 **49GB**（当前使用约44GB，剩余约5GB）
- **架构**：基于 Ampere 架构，支持张量并行和高效内存管理
- **CUDA 版本**：12.4，兼容 vLLM 的优化需求

### 二、Qwen3-1.7b 模型特性

- **参数规模**：17亿参数（1.7B）
- **内存需求**：
  - 全精度（FP32）：约34GB（1.7B×4B）
  - 半精度（FP16）：约17GB（1.7B×2B）
  - vLLM 支持的量化格式（如 INT8/INT4）：可进一步压缩至8.5GB~4.25GB
- **计算特性**：1.7B 模型属于轻量级大模型，适合在单卡或双卡上高效部署

### 三、vLLM 优化技术对吞吐量的影响

vLLM 通过以下技术提升吞吐量：

1. **PagedAttention**：高效管理KV缓存，支持动态批处理
2. **CUDA 内核优化**：针对Transformer架构深度优化计算流程
3. **张量并行**：支持多GPU自动切分模型，提升并行度
4. **动态批处理**：根据请求动态调整批大小，最大化GPU利用率

### 四、理论吞吐量估算

#### 1. 内存分配策略（双卡部署）

- **模型并行方案**：将模型均匀切分到2张GPU，每张卡承担约0.85B参数
- **显存占用**：
  - FP16 模式：约8.5GB（0.85B×2B）+ 缓存/开销 ≈10GB/卡
  - INT8 模式：约4.25GB（0.85B×1B）+ 开销 ≈6GB/卡
  - 剩余显存可用于批量推理和KV缓存（每卡约40GB可用）

#### 2. 吞吐量关键指标（基于类似模型基准）

| 场景                | 批大小 | 序列长度 | 吞吐量（token/s） | 备注                          |
|---------------------|--------|----------|-------------------|-------------------------------|
| FP16 单卡           | 16     | 512      | 800-1200           | 标准精度，适合高质量推理      |
| FP16 双卡张量并行   | 32     | 512      | 1800-2400         | 并行度提升，吞吐量近线性增长  |
| INT8 双卡            | 64     | 512      | 2800-3500         | 量化后计算效率显著提升        |
| INT4 双卡（实验性） | 128    | 512      | 4000+             | 极限量化，可能牺牲少量精度    |

#### 3. 理论上限推算

- **峰值计算能力**：L20 单卡算力约24 TFLOPS（FP16），双卡约48 TFLOPS
- **Qwen3-1.7b 计算密度**：每token推理约需34GFLOPS（1.7B×20×FP16）
- **理论最大吞吐量**：48 TFLOPS ÷ 34 GFLOPS/token ≈ **1411 token/s**（FP16）
  - 实际因优化和开销，峰值约为理论值的60%-80%，即 **850-1130 token/s**（单卡FP16）
  - 双卡并行可提升至 **1700-2260 token/s**（FP16），量化后可再提升2-3倍

### 五、影响吞吐量的关键因素

1. **批处理大小**：增大批大小可提升吞吐量，但受限于显存（如批大小128在INT4下可行，FP16可能只能到32）。
2. **序列长度**：长序列（如2048 token）会占用更多KV缓存，降低并发能力。
3. **量化精度**：INT8/INT4可显著提升吞吐量，但需平衡精度损失。
4. **Docker 配置**：需确保容器获得足够的GPU资源（如使用`nvidia-container-toolkit`）。

### 六、优化建议

1. **量化部署**：使用INT8或INT4量化，在保持可接受精度的前提下提升吞吐量。
2. **动态批处理**：利用vLLM的动态批处理特性，自动合并小请求以提高GPU利用率。
3. **模型并行**：合理切分模型到两张GPU，避免显存瓶颈。
4. **硬件加速**：启用Tensor Core（Ampere架构特性），提升矩阵运算效率。

### 七、总结

在2×NVIDIA L20（49GB显存）的配置下，使用vLLM部署Qwen3-1.7b的**理论吞吐量范围**为：

- **FP16 双卡**：1800-2400 token/s（标准精度，平衡质量与性能）
- **INT8 双卡**：2800-3500 token/s（量化部署，适合多数业务场景）
- **INT4 双卡**：4000+ token/s（极限优化，适合对精度要求不高的场景）

实际部署时，建议通过vLLM的基准测试工具（如 `vllm benchmark`）进行实测，并根据业务需求调整批大小和量化策略。

| 模型         | 上下文长度        | 可并发请求数  |
| ---------- | ------------ | ------- |
| Qwen3-1.7B | 512 tokens   | 200+    |
| Qwen3-1.7B | 1024 tokens  | 120~150 |
| Qwen3-1.7B | 2048 tokens  | 60~80   |
| Qwen3-1.7B | 4096 tokens  | 30~40   |
| Qwen3-1.7B | 8192 tokens  | 10~16   |
| Qwen3-1.7B | 32768 tokens | 1~2     |

| 模型         | 上下文长度       | 两张 L20 支持并发量 |
| ---------- | ----------- | ------------ |
| Qwen3-1.7B | 2048 tokens | 100~150      |
| Qwen3-1.7B | 4096 tokens | 40~60        |
| Qwen3-1.7B | 8192 tokens | 20~30        |
