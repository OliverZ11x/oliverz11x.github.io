---
date created: 2024/8/29 16:9
date modified: 2024/8/29 16:15
---
### 模型预测概述

在本次实验中，我们使用了 PaddleNLP 提供的 `Taskflow` 工具进行零样本文本分类任务。具体步骤如下：

1. **初始化分类任务**：
   - 首先，读取存储在 `label.txt` 中的分类标签，将其加载为一个列表 `schema`，然后使用 `Taskflow` 初始化零样本文本分类模型 `my_cls`，模型使用了 `utc-base`，并且由于设备性能限制，选择在 CPU上进行预测。

   ```python
   from paddlenlp import Taskflow
   import json
   from tqdm import tqdm

   # 读取 schema 文件
   with open('./data/3label.txt', 'r') as file:
       schema = [line.strip() for line in file]

   # 初始化分类任务
   my_cls = Taskflow("zero_shot_text_classification", model="utc-base", schema=schema, device_id=-1)
   ```

2. **逐行读取测试文件**：
   - 使用生成器函数逐行读取 `filtered_sentences_CN.txt` 文件中的句子，并对空行进行过滤。

   ```python
   # 定义生成器逐行读取文件
   def read_lines(file_path):
       with open(file_path, 'r', encoding='utf-8') as file:
           for line in file:
               stripped_line = line.strip()
               if stripped_line:
                   yield stripped_line
   ```

3. **分批处理数据**：
   - 为了防止内存占用过高，定义每批次处理 128 行文本，逐批对数据进行预测。
   - 使用 `pbar` 显示进度条，跟踪处理进度，并确保所有文本都能被处理。

   ```python
   # 分批处理数据
   batch_size = 128  # 每批处理的行数
   results = []

   # 计算文件总行数
   total_lines = sum(1 for _ in read_lines('./data/test.txt'))

   # 使用 tqdm 显示进度条
   with tqdm(total=total_lines, desc="Processing") as pbar:
       batch = []
       for line in read_lines('./data/test.txt'):
           batch.append(line)
           if len(batch) == batch_size:
               batch_result = my_cls(batch)
               results.extend(batch_result)
               batch = []
           pbar.update(1)

       # 处理剩余的行
       if batch:
           batch_result = my_cls(batch)
           results.extend(batch_result)
           pbar.update(len(batch))
   ```

4. **保存预测结果**：
   - 将分类结果保存为 `JSON` 格式的文件，以便后续分析和使用。

   ```python
   # 将结果保存为 JSON 文件
   with open('./result/result.json', 'w', encoding='utf-8') as f:
       json.dump(results, f, ensure_ascii=False, indent=4)
   ```

### 性能优化建议

由于目前在内存和设备性能有限的条件下使用 CPU 进行预测，速度相对较慢。建议在内存和设备性能较好的平台上，可以通过以下方式优化模型预测性能：

1. **使用 GPU 加速**：在支持 GPU 的环境中，指定 `device_id` 为 GPU 设备 ID，可以显著提升模型预测速度。
2. **并行处理**：在性能较好的平台上可以采用并行处理方式，进一步加快模型预测过程。