---
date created: 2025/5/26 13:41
date modified: 2025/5/26 13:43
---
```python
import asyncio
from fastapi import FastAPI, HTTPException
from typing import Any, Dict, Optional
from contextlib import AsyncExitStack
import os
import traceback # Added for more detailed error logging

# Added imports for OpenAI
from openai import OpenAI

# 启用真实的 MCP 客户端库导入
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client

# --- OpenAI API 密钥 (硬编码 - 存在严重安全风险！) ---
openai_api_key = "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"
openai_client = OpenAI(api_key=openai_api_key)

# 假设 'mcp' 库已安装。如果未安装，以下导入将失败。
# 用户需要确保在其环境中安装了必要的 MCP 客户端库。
# from mcp import ClientSession, StdioServerParameters  # 'mcp' 库的核心组件
# from mcp.client.stdio import stdio_client           # 用于 stdio 通信

# --- 开始: 从用户提供的MCP客户端示例中提取的必要类和函数的模拟/占位符 ---
# 注意: 这部分是为了让代码结构完整。在实际环境中，您应该安装并使用官方的 'mcp' 库。
# 如果 'mcp' 库可用，则应删除以下模拟定义，并取消上面真实导入的注释。

# class StdioServerParameters:
#     def __init__(self, command: str, args: list[str], env: Optional[dict] = None, cwd: Optional[str] = None):
#         self.command = command
#         self.args = args
#         self.env = env or {}
#         self.cwd = cwd or os.getcwd()

# class MockStdioTransport:
#     async def __aenter__(self):
#         # 模拟进入上下文
#         print(f"MockStdioTransport: Entered context for server.")
#         # 模拟 subprocess.Popen
#         # 在真实场景中，这里会启动服务器进程
#         # self.process = await asyncio.create_subprocess_exec(...)
#         # return (mock_reader, mock_writer)
#         # 这里简化，不实际创建子进程或流
#         # print("Mock: Returning (None, None) for reader, writer") # Debug print
#         return (None, None) # 模拟 reader 和 writer

#     async def __aexit__(self, exc_type, exc_val, exc_tb):
#         # 模拟退出上下文
#         print(f"MockStdioTransport: Exited context.")
#         # if hasattr(self, 'process') and self.process.returncode is None:
#         #     self.process.terminate()
#         #     await self.process.wait()

# def stdio_client(params: StdioServerParameters):
#     print(f"Mock stdio_client called with: cmd='{params.command} {' '.join(params.args)}'")
#     # 返回一个异步上下文管理器
#     return MockStdioTransport()

# class ClientSession:
#     def __init__(self, reader, writer): # reader 和 writer 在模拟中为 None
#         self.reader = reader
#         self.writer = writer
#         self._initialized = False
#         print("Mock ClientSession: Initialized.")

#     async def __aenter__(self):
#         print("Mock ClientSession: Entered context.")
#         return self

#     async def __aexit__(self, exc_type, exc_val, exc_tb):
#         print("Mock ClientSession: Exited context.")

#     async def initialize(self):
#         print("Mock ClientSession: initialize() called.")
#         # 实际实现中会与服务器握手
#         await asyncio.sleep(0.01) # 模拟异步操作
#         self._initialized = True
#         print("Mock ClientSession: Initialized session with server.")

#     async def list_tools(self):
#         if not self._initialized:
#             raise RuntimeError("Session not initialized.")
#         print("Mock ClientSession: list_tools() called.")
#         # 模拟从服务器获取工具列表
#         # 这应该返回一个包含 Tool 对象（或类似结构）的响应
#         class MockTool:
#             def __init__(self, name, description="Mock tool description", inputSchema=None):
#                 self.name = name
#                 self.description = description
#                 self.inputSchema = inputSchema or {}

#         class MockListToolsResponse:
#             def __init__(self, tools):
#                 self.tools = tools

#         await asyncio.sleep(0.01) # 模拟异步操作
#         return MockListToolsResponse(tools=[MockTool(name="get_user_info"), MockTool(name="get_user_posts")])


#     async def call_tool(self, tool_name: str, args: Dict[str, Any]):
#         if not self._initialized:
#             raise RuntimeError("Session not initialized.")
#         print(f"Mock ClientSession: call_tool() called for tool '{tool_name}' with args {args}.")
#         # 模拟调用服务器工具并获取结果
#         # 实际实现会向服务器发送请求并接收响应
#         await asyncio.sleep(0.05) # 模拟异步操作，稍长一些

#         class MockCallToolResponse:
#             def __init__(self, content):
#                 self.content = content

#         # 根据 tool_name 和 args 返回模拟数据
#         # 假设 get_user_info 返回更结构化的数据以便于总结
#         if tool_name == "get_user_info" and "uniqueId" in args:
#             user_id = args["uniqueId"]
#             mock_data = {
#                 "id": user_id,
#                 "username": user_id,
#                 "display_name": f"User {user_id.capitalize()}",
#                 "followers": 12345,
#                 "following": 678,
#                 "bio": f"This is a mock bio for user {user_id}. They are interested in mock data and testing.",
#                 "is_verified": True,
#                 "join_date": "2023-01-15"
#             }
#             return MockCallToolResponse(content=mock_data)
#         elif tool_name == "get_user_posts" and "uniqueId" in args:
#              user_id = args["uniqueId"]
#              mock_posts = [
#                  {"id": f"{user_id}_post_1", "text": f"Mock post 1 by {user_id}. #mock #test", "likes": 100, "comments": 10},
#                  {"id": f"{user_id}_post_2", "text": f"Mock post 2 by {user_id}. Testing the API.", "likes": 50, "comments": 5}
#              ]
#              return MockCallToolResponse(content=mock_posts)

#         # Fallback for other tools
#         return MockCallToolResponse(content={"status": "mock success", "tool": tool_name, "args": args})

# --- 结束: MCP客户端库组件的模拟/占位符 ---


app = FastAPI(title="MCP Client API", version="0.2.1") # Updated version

# 配置各个MCP服务器脚本的路径
# 这些应该是用户环境中实际的服务器脚本路径
MCP_SERVER_SCRIPTS = {
    "tiktok": "/home/ubuntu/twmcp/tiktok_mcp_server.py",
    "linkedin": "/home/ubuntu/twmcp/linkedin_mcp_server.py", # 假设路径
    "twitter": "/home/ubuntu/twmcp/twitter_mcp_server.py",   # 假设路径
    "contentunderstanding": "/home/ubuntu/twmcp/contentunderstanding_mcp_server.py" # 假设路径
}

# 用于与 MCP 服务器通信的函数
async def call_mcp_tool_via_protocol(platform: str, tool_name: str, params: Dict[str, Any]) -> Dict[str, Any]:
    server_script_path = MCP_SERVER_SCRIPTS.get(platform)
    if not server_script_path:
        raise HTTPException(status_code=404, detail=f"MCP server script for platform '{platform}' not configured.")

    if not os.path.exists(server_script_path):
        raise HTTPException(status_code=500, detail=f"MCP server script not found at path: {server_script_path}")

    # Determine if GPT processing is requested and extract instructions
    gpt_instructions = params.pop("__gpt_process__", None)

    # 确定执行服务器脚本的命令 (python 或 node)
    # 假设都是 python 脚本
    command = "python"
    if server_script_path.endswith(".js"):
        command = "node" # 如果有 Node.js 服务器脚本

    server_params = StdioServerParameters(
        command=command,
        args=[server_script_path],
        # env=os.environ.copy(), # 可以传递当前环境或自定义环境
        # cwd=os.path.dirname(server_script_path) # 可以设置工作目录
    )

    mcp_tool_result_content = None # Variable to store the MCP tool's result content

    try:
        async with AsyncExitStack() as exit_stack:
            # 1. 启动服务器并通过 stdio 连接
            #    stdio_client 返回一个异步上下文管理器，提供 (reader, writer) 对
            # 在模拟中 stdio_client 返回 MockStdioTransport
            stdio_transport_manager = stdio_client(server_params)
            # 进入 stdio_client 上下文，模拟获取 reader/writer
            # 在 MockStdioTransport 中，__aenter__ 返回 (None, None)
            stdio_transport = await exit_stack.enter_async_context(stdio_transport_manager)
            # stdio_transport is (None, None) in mock, (reader, writer) in real
            stdio_reader, stdio_writer = stdio_transport # This unpacks (None, None) in mock

            # 2. 创建并初始化 ClientSession
            #    ClientSession 也使用异步上下文管理
            # Pass the potentially None reader/writer to ClientSession
            async with ClientSession(stdio_reader, stdio_writer) as session:
                await session.initialize()

                # 可选: 列出可用工具以进行调试或验证
                # available_tools_response = await session.list_tools()
                # print(f"Available tools on {platform}: {[tool.name for tool in available_tools_response.tools]}")

                # 3. 调用工具
                # Pass the modified params (without __gpt_process__) to the MCP tool
                tool_result = await session.call_tool(tool_name, params)

                # Assume tool_result.content is the dictionary we want to return or process
                if hasattr(tool_result, 'content'):
                    mcp_tool_result_content = tool_result.content
                else:
                    # Fallback if 'content' attribute is missing (adapt as per actual mcp library)
                    mcp_tool_result_content = {"raw_result": str(tool_result)}

        # --- Start GPT Processing if requested ---
        if gpt_instructions and openai_client:
            print(f"Attempting to process MCP tool result with GPT using instructions: {gpt_instructions}")
            try:
                # Build the prompt for GPT
                # We can make this more sophisticated based on gpt_instructions['task']
                # For now, a simple prompt incorporating instructions and data
                prompt_messages = [
                    {"role": "system", "content": "You are a helpful assistant that processes and refines data provided by external tools."},
                    {"role": "user", "content": f"Here is the data from the tool call:\n\n{mcp_tool_result_content}\n\nProcess this data according to the following instructions:\n\n{gpt_instructions}"}
                ]

                # Call OpenAI API
                # Using gpt-4o-mini as gpt-4.1-nano is not a standard model name
                gpt_completion = openai_client.chat.completions.create(
                    model="gpt-4o-mini", # Using gpt-4o-mini
                    messages=prompt_messages,
                    max_tokens=2000, # Adjust max tokens as needed
                    temperature=0.7 # Adjust temperature for creativity vs adherence
                )

                # Extract GPT's response
                if gpt_completion.choices and gpt_completion.choices[0].message.content:
                    gpt_processed_result = gpt_completion.choices[0].message.content
                    print("GPT processing successful.")
                    # Return the GPT processed result, perhaps formatted
                    # Returning as a dictionary with the processed content
                    return {"gpt_processed_result": gpt_processed_result, "original_mcp_result": mcp_tool_result_content}
                else:
                    print("GPT processing failed: No content in response.")
                    # If GPT processing fails or returns no content, return the original MCP result
                    return mcp_tool_result_content

            except Exception as gpt_e:
                print(f"Error during GPT processing: {type(gpt_e).__name__} - {str(gpt_e)}")
                traceback.print_exc()
                # If GPT processing fails, return the original MCP result and maybe an error message
                return {"mcp_result": mcp_tool_result_content, "gpt_processing_error": str(gpt_e)}
        elif gpt_instructions and not openai_client:
             print("GPT processing requested but OpenAI client not initialized (API key missing?). Returning original MCP result.")
             return mcp_tool_result_content
        else:
            # --- End GPT Processing ---
            # If no GPT processing was requested, return the original MCP tool result
            print("No GPT processing requested. Returning original MCP result.")
            return mcp_tool_result_content


    except FileNotFoundError:
        # StdioServerParameters 的 command 可能未找到
        raise HTTPException(status_code=500, detail=f"Command '{command}' not found. Ensure it's in PATH.")
    except ConnectionRefusedError: # 或其他特定于 stdio_client 的连接错误
        raise HTTPException(status_code=503, detail=f"Failed to connect to MCP server for {platform} at {server_script_path}.")
    except RuntimeError as e: # 例如 "Session not initialized" 或其他 mcp 库运行时错误
        print(f"MCP Runtime error for {platform} ({tool_name}): {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"MCP client runtime error: {str(e)}")
    except Exception as e:
        # 捕获其他意外错误
        print(f"Unexpected error calling MCP service {platform} ({tool_name}): {type(e).__name__} - {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {str(e)}")


@app.post("/{platform}/{tool_name}", summary="Call an MCP tool on a specified platform and optionally process with GPT")
async def call_mcp_tool_endpoint(platform: str, tool_name: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    通用端点，用于调用指定平台上的MCP工具。
    平台名称和工具名称在路径中指定。
    工具参数在请求体中作为JSON对象提供。
    可以在payload中包含 "__gpt_process__": {...} 来指定对工具结果进行GPT处理。
    """
    if platform not in MCP_SERVER_SCRIPTS:
        raise HTTPException(status_code=404, detail=f"Platform '{platform}' is not configured or supported.")

    try:
        # 'payload' 就是工具的参数，可能包含 __gpt_process__
        # call_mcp_tool_via_protocol 会处理并移除 __gpt_process__ 键
        return await call_mcp_tool_via_protocol(platform=platform, tool_name=tool_name, params=payload)
    except HTTPException as e:
        raise e # Re-raise handled HTTPException
    except Exception as e:
        # Catch other unexpected errors
        print(f"Client API error for {platform}/{tool_name}: {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Client API error: {str(e)}")

# 为了保持向后兼容性或特定性，可以保留旧的/tiktok/user_info路由，
# 但让它调用新的通用端点逻辑或直接调用 call_mcp_tool_via_protocol
@app.get("/tiktok/user_info", summary="Get TikTok User Info (via GET, specific endpoint)")
async def tiktok_user_info_get(unique_id: str) -> Dict[str, Any]:
    """
    通过MCP客户端获取TikTok用户信息 (GET请求的特定端点)。
    注意: GET请求通常不带请求体，因此无法直接在URL参数中指定复杂的GPT处理指令。
    此端点将只返回原始的MCP工具结果。如需GPT处理，请使用POST端点。
    """
    try:
        # unique_id 是查询参数，params 是传递给 call_mcp_tool_via_protocol 的字典
        # 在GET请求中，我们通常不进行GPT处理，或者需要通过URL参数传递简单的指令（这里未实现）
        # 为了简化，GET端点只调用MCP工具并返回原始结果
        return await call_mcp_tool_via_protocol(platform="tiktok", tool_name="get_user_info", params={"uniqueId": unique_id})
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"Client API error for /tiktok/user_info (GET): {str(e)}")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Client API error: {str(e)}")


if __name__ == "__main__":
    import uvicorn
    print("Starting MCP Client API server on http://localhost:8080")
    print("Access OpenAPI docs at http://localhost:8080/docs")
    print("\nExample POST request to generic endpoint (without GPT processing):")
    print("""curl -X POST -H "Content-Type: application/json" -d '{"uniqueId": "someuser"}' http://localhost:8080/tiktok/get_user_info""")
    print("\nExample POST request to generic endpoint (with GPT processing):")
    print("""curl -X POST -H "Content-Type: application/json" -d '{"uniqueId": "someuser", "__gpt_process__": {"task": "summarize_user_info", "language": "Chinese", "format": "bullet_points", "instructions": "Please summarize the user information in bullet points in Chinese."}}' http://localhost:8080/tiktok/get_user_info""") # Added example instructions

    # 注意: MCP_SERVER_SCRIPTS 中的路径需要是您环境中实际的脚本路径。
    # 确保 tiktok_mcp_server.py 等脚本存在且可由 python 命令执行。
    # 另外，确保您已安装 'mcp' 客户端库 (或此处使用的模拟版本能按预期工作)，
    # 并且已安装 'openai' 和 'python-dotenv' (`pip install openai python-dotenv`)，
    # 且已设置 OPENAI_API_KEY 环境变量。

    uvicorn.run(app, host="0.0.0.0", port=8080) 
```