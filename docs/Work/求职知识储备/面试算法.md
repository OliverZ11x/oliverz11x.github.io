---
title: 面试算法
date created: 2024/7/31 11:15
date modified: 2025/4/11 15:24
---
# 无监督算法

聚类和降维

k-means和PCA

EM算法，k怎么确定

---

### k-means算法

K均值（K-means）聚类是一种常用的无监督聚类算法，其主要思想是将数据点分为K个簇，使得每个数据点都属于离它最近的簇的中心点所代表的簇。以下是K均值聚类的基本步骤：

1. **初始化**：随机选择K个数据点作为初始的簇中心。
2. **分配数据点**：对于每个数据点，根据其与各个簇中心的距离，将其分配到最近的簇中心所代表的簇。
3. **更新簇中心**：对每个簇，计算其所有数据点的均值，将该均值作为新的簇中心。
4. **重复步骤2和3**：迭代执行步骤2和3，直到簇中心不再发生显著变化或达到预定的迭代次数。

K均值聚类的优点包括简单易实现、计算效率高；缺点则包括对初始簇中心敏感，可能收敛到局部最优解，对离群点敏感等。在实际应用中，通常需要根据具体数据集的特点来选择适当的K值和优化算法参数。

### K-means聚类中K怎么确定？

确定K值是K均值聚类中的一个关键问题。一种常用的方法是使用肘部法则（elbow method）。肘部法则通过尝试不同的K值进行K均值聚类，并计算每个K值下的聚类误差（如SSE，簇内平方和），然后绘制K值与聚类误差的关系图。通常会发现随着K值的增加，聚类误差会逐渐减小，但在某个K值后，聚类误差的下降速度会急剧减缓，形成一个肘部。这个肘部对应的K值通常被认为是一个合适的选择。

另外，对于特定的应用领域，也可以借助领域知识或者实际需求来确定K值。例如，如果已经明确知道要分成的类别数量，那么K值就可以直接设定为该数量。

除了肘部法则，还有一些其他方法用来确定K值，如轮廓系数（silhouette score）和Gap统计量（Gap statistic），这些方法也可以作为参考。最终选择K值时，需要综合考虑具体数据集的特点和实际需求。

### k-meas算法有什么缺点？如何改进？

1. 对初始中心点敏感：K均值算法对初始中心点的选择很敏感，不同的初始中心点可能导致不同的聚类结果，甚至可能收敛到局部最优解。
2. 对异常值敏感：K均值算法对异常值敏感，异常值可能会影响簇中心的计算。
3. 处理不规则形状的簇困难：K均值算法假定簇是凸形的，对于非凸形状的簇，K均值算法的表现可能不佳。
4. 难以处理不同密度的簇：K均值算法难以处理不同密度的簇，因为它假定所有簇的密度相似。

K均值算法对初始值选取敏感的问题可以通过以下方式进行改进：

1. **多次随机初始化**：可以多次运行K均值算法，每次使用不同的随机初始化来获得不同的聚类结果，然后选择最优的结果。这样可以减少初始值选择的影响。
2. **K均值++算法**：K均值++算法是一种改进的初始化方法，它可以更好地选择初始簇中心，从而降低收敛到局部最优解的风险。
3. **谱聚类**：谱聚类是一种基于图论的聚类方法，它不需要显式地指定簇的数量，因此不需要初始值。谱聚类可以处理不规则形状的簇，并且对初始值不敏感。
4. **密度聚类算法**：与K均值算法相比，密度聚类算法（如DBSCAN）对初始值的选择不太敏感，并且可以有效处理不同密度的簇以及噪声点。

---

# PCA？？？？

---

# 逻辑回归

损失函数

极大似然和逻辑回归有什么关系

GD2阶

# softmax

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36.png)

# 树模型

---

# 对于机器学习模型的理解

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-1.png)

---

# 模型损失函数

回归和分类

### 回归

L2 loss(mse)

### 分类

logloss

cross-entropy loss

hinge loss

# 模型评估

### 评估方法

### 评估准则

---

# 高频面试题

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-2.png)

1. 如何解决正负样本不均衡？

## Transformer

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-3.png)

为什么要对qkv进行投影？

不投影attention层没有可训练参数，会让相同的qkv点积，导致每个词的注意力都在自己身上。

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-4.png)

---

### attention计算

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-5.png)

为什么要分为不同的注意力头（num_attention_heads）?

CNN中多通道思想，希望不同的注意力头学到不同的特征。

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-6.png)

attention有加性和乘性，为什么不用加性attention？

计算机对于乘性效果更好

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-7.png)

除以标准差根号d的作用是：变回均值为0方差为1的正态分布

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-8.png)

mask为什么为-10000？

这是为了在softmax函数中使得被mask掉的位置的概率趋近于0。注意softmax的公式。

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-9.png)

---

### 残差连接和normalization

transformer中为什么要用layernorm，和batchnorm有什么区别？

在Transformer模型中使用Layer Normalization（LN）是为了在每个特征维度上独立地进行标准化，以减少内部协变量转移（Internal Covariate Shift）并加速训练过程。不同于Batch Normalization（BN），LN不是在batch维度上进行标准化，而是在特征维度上进行标准化。这样做有助于保持每个特征维度的分布稳定性，避免了BN在处理变长序列时可能引入的问题。

另外，LN相较于BN更适合用于RNN和Transformer等序列模型中，因为LN不需要在整个batch上进行统计，而是在每个样本上进行统计，因此更适应于**变长序列**的处理。此外，LN对于小批量的数据更稳定，而BN对小批量的数据可能会引入较大的噪声。

总的来说，LN和BN都是用来加快训练速度，减少内部协变量转移的方法，但在应用场景和计算方式上有所不同。

transformer中为什么要用prenorm和postnorm有什么区别?

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-10.png)

---

### FFN

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-11.png)
