---
date created: 2025/7/18 9:41
date modified: 2025/7/18 10:40
---

非常好，有了你补充的这些信息，我将交接表的**模型部署命令说明部分**与**定制化开发指引**进一步丰富，使得接手人不仅能“跑起来”，还能理解模型的运行机制与代码入口，便于后续维护与开发。

以下是更新后的关键内容片段，将会嵌入前述交接表中第 **二节** 和 **第六节**：

---

## 二（更新）. 容器部署与运行细节（增强版）

### ✅ 通用模型（Qwen3-14B）完整部署命令示例：

```bash
sudo docker run -d --runtime=nvidia --gpus all \
-v /home/ubuntu/.cache/modelscope/hub/models/Qwen/Qwen3-14B:/Qwen/Qwen3-14B \
-v /home/ubuntu/code/vllm/chat_template:/chat_template \
-p 8000:8000 \
--ipc=host \
--log-driver json-file \
--log-opt max-size=10m \
--log-opt max-file=3 \
--name vllm-general \
vllm/vllm-openai:latest \
--model /Qwen/Qwen3-14B \
--tensor-parallel-size 4 \
--api-key token-abc123 \
--dtype 'float16' \
--port 8000 \
--chat-template /chat_template/template_custom.jinja \
--no-enable-chunked-prefill \
--rope-scaling '{"rope_type":"yarn","factor":4.0,"original_max_position_embeddings":32768}' \
--max-model-len 131072
```

> 🔗 **更多参数说明与定制部署参考：**

- vLLM 推理引擎完整参数列表文档：
	[https://vllm.hyper.ai/docs/inference-and-serving/engine_args](https://vllm.hyper.ai/docs/inference-and-serving/engine_args)
	
- OpenAI 兼容接口说明文档：
	[https://vllm.hyper.ai/docs/inference-and-serving/openai_compatible_server](https://vllm.hyper.ai/docs/inference-and-serving/openai_compatible_server)

---

## 六（新增）. vLLM 定制化开发路径说明

如需定制模型推理行为（如动态路由、多任务指令解析、Token 重写、预处理优化等），请修改以下路径中的源码文件：

```python
源码路径：/home/ubuntu/code/vllm/vllm/engine/
```

**建议优先修改的关键文件模块说明如下：**

|文件/模块名称|功能说明|
|---|---|
|`vllm/engine/llm_engine.py`|核心推理引擎类，模型加载、请求分发入口|
|`vllm/engine/async_llm_engine.py`|异步推理核心，支持多请求异步调度|
|`vllm/engine/arg_utils.py`|CLI 参数解析模块，可添加自定义启动参数|
|`vllm/engine/openai_server.py`|OpenAI 接口兼容层，修改接口逻辑或增加日志审计点|
|`vllm/engine/routing/`|各类请求的分发、调度逻辑模块|

**开发建议**：

- 修改后使用 `docker build` 自行构建定制镜像；
	
- 如仅调试可挂载本地代码目录进行热更新；
	
- 推荐建立 Git 分支记录开发变更，避免污染官方分支。

---

是否需要我将更新后的 **完整交接表导出为 Word 文档** 或提供 **定制化部署版本的 Dockerfile 模板**？我可以一并帮你打包整理。