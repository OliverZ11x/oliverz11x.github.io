---
date created: 2025/4/3 9:36
date modified: 2025/4/7 13:34
---

Unsloth 目前不支持 **单个批次（batch）跨多张 GPU 并行计算**，但可以通过 **模型并行（tensor parallelism）** 或 **数据并行（data parallelism）** 实现部分多卡加速。

### 1. Unsloth 的默认推理方式

Unsloth 使用的是 **Flash Attention 2** 和 **Paged Attention** 来加速推理，通常是将整个模型加载到 **一张 GPU** 上，并行化主要体现在 **序列并行**（例如 KV Cache 高效存取）。

### 2. 多 GPU 方案

如果你的需求是 **让两张卡同时并行输出（提高吞吐量）**，可以考虑以下方案：

#### **（1）手动分配 batch 到不同的 GPU**

你可以显式地将不同的输入分配到不同的 GPU：

```python
import torch
from unsloth import FastLanguageModel

# 加载模型到 GPU 0
model_0 = FastLanguageModel.from_pretrained("unsloth/Qwen1.5-7B", torch_dtype=torch.float16, device_map="cuda:0")

# 加载模型到 GPU 1
model_1 = FastLanguageModel.from_pretrained("unsloth/Qwen1.5-7B", torch_dtype=torch.float16, device_map="cuda:1")

# 分别在两张卡上推理
input_0 = "你好，请介绍一下人工智能"
input_1 = "Hello, what is the capital of France?"

output_0 = model_0.generate(input_0)
output_1 = model_1.generate(input_1)
```

这样可以并行推理 **不同 batch**，但 **不能并行计算同一 batch**。

### 3. 结论

- **Unsloth 默认是单 GPU 推理，无法自动并行计算同一个 batch**。
- **可以手动加载到不同 GPU 上，让每张卡处理不同的输入**，适用于并行处理多请求场景。
- **想在多 GPU 上并行计算同一个 batch，需要使用 DeepSpeed 或 FSDP**，但需要手动配置。

---

![[IMG-2025-04-03-15-04-13.png]]

![[IMG-2025-04-03-15-07-07.png]]

普通对话接口

接口地址： POST `http://10.10.10.41:8000/api/chat`

**请求头**: `Content-Type: application/json`

**请求参数**（JSON）:

```json
{
    "message": "你好"
}
```

**响应参数**（JSON）:

```json
{
    "response": "你好，很高兴为你提供帮助。<|im_end|>"
}
```

```python
curl --location 'http://10.10.10.41:8000/api/chat' \
--header 'Content-Type: application/json' \  
--data '{  
    "message":"你好"  
}'


curl --location 'http://10.10.10.41:8000/api/chat' --header 'Content-Type: application/json' --data '{"message":"你好"}'
```