---
date created: 2025/3/19 9:53
date modified: 2025/3/20 14:17
---

```python
import copy
import os
import logging
import random
import numpy as np
import torch
import torch.distributed
import transformers
from dataclasses import dataclass, field
from typing import Optional, Dict, Sequence
from transformers import Trainer, BitsAndBytesConfig
from datasets import load_dataset
from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training, PeftModel
from peft.tuners.lora import LoraLayer
from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR

# 常量定义
IGNORE_INDEX = -100
EOT_TOKEN = "<|EOT|>"
logger = logging.getLogger(__name__)

# 构建指令提示
def build_instruction_prompt(instruction: str) -> str:
    return f'''
你是一个人工智能助手，由Xmindai公司微调。
### Instruction:
{instruction.strip()}
### Response:
'''.lstrip()

# 参数类定义
@dataclass
class ModelArguments:
    trainable: Optional[str] = field(default="q_proj,v_proj,k_proj,o_proj,gate_proj,down_proj,up_proj")
    lora_rank: Optional[int] = field(default=8)
    lora_dropout: Optional[float] = field(default=0.1)
    lora_alpha: Optional[float] = field(default=32.0)
    modules_to_save: Optional[str] = field(default="embed_tokens,lm_head")
    use_lora: Optional[bool] = field(default=False)
    model_name_or_path: Optional[str] = field(default="microsoft/Phi-3.5-MoE-instruct")
    attn_implementation: Optional[str] = field(default="flash_attention_2")
    double_quant: bool = field(default=True)
    quant_type: str = field(default="nf4")
    bits: int = field(default=16)

@dataclass
class DataArguments:
    data_path: str = field(default=None, metadata={"help": "Path to the training data."})

@dataclass
class TrainingArguments(transformers.TrainingArguments):
    cache_dir: Optional[str] = field(default=None)
    optim: str = field(default="adamw_torch")
    model_max_length: int = field(default=512)

# 回调类定义
class SavePeftModelCallback(transformers.TrainerCallback):
    """保存PEFT模型的回调"""

    def __init__(self, tokenizer=None):
        super().__init__()
        self.tokenizer = tokenizer

    def save_model(self, args, state, kwargs):
        logger.info('Saving PEFT checkpoint...')
        checkpoint_folder = (
            os.path.join(state.best_model_checkpoint, "adapter_model")
            if state.best_model_checkpoint
            else os.path.join(args.output_dir, f"{PREFIX_CHECKPOINT_DIR}-{state.global_step}")
        )

        peft_model_path = os.path.join(checkpoint_folder, "adapter_model")
        kwargs["model"].save_pretrained(peft_model_path)

        if self.tokenizer:
            self.tokenizer.save_pretrained(peft_model_path)

    def on_save(self, args, state, control, **kwargs):
        self.save_model(args, state, kwargs)
        return control

    def on_train_end(self, args, state, control, **kwargs):
        with open(os.path.join(args.output_dir, 'completed'), 'a'):
            os.utime(os.path.join(args.output_dir, 'completed'))
        self.save_model(args, state, kwargs)

class LRMonitorCallback(transformers.TrainerCallback):
    """学习率监控回调"""

    def __init__(self):
        self.prev_loss = float('inf')

    def on_log(self, args, state, control, logs=None, **kwargs):
        if args.local_rank == 0 and logs:
            current_loss = logs.get('loss', None)
            if current_loss is not None:
                lr = logs.get('learning_rate', 0)
                logger.info(f"\n当前训练状态:")
                logger.info(f"Step: {state.global_step}")
                logger.info(f"Loss: {current_loss:.4f} (变化: {self.prev_loss - current_loss:.4f})")
                logger.info(f"Learning Rate: {lr:.6f}")
                logger.info(f"Epoch: {logs.get('epoch', 0):.2f}\n")
                self.prev_loss = current_loss

# 检查点处理
def get_last_checkpoint(checkpoint_dir):
    """获取最后的检查点"""
    if os.path.isdir(checkpoint_dir):
        if os.path.exists(os.path.join(checkpoint_dir, 'completed')):
            return None  # 训练已完成
        max_step = max(
            (int(filename.replace(PREFIX_CHECKPOINT_DIR + '-', ''))
             for filename in os.listdir(checkpoint_dir)
             if os.path.isdir(os.path.join(checkpoint_dir, filename)) and filename.startswith(PREFIX_CHECKPOINT_DIR)),
            default=0
        )
        if max_step == 0:
            return None
        return os.path.join(checkpoint_dir, f'{PREFIX_CHECKPOINT_DIR}-{max_step}')
    return None

# 安全保存模型
def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str):
    """将模型保存到磁盘"""
    if trainer.args.should_save:
        state_dict = trainer.model.state_dict()
        cpu_state_dict = {key: value.cpu() for key, value in state_dict.items()}
        trainer._save(output_dir, state_dict=cpu_state_dict)

# 数据预处理
def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:
    """将字符串序列进行分词处理"""
    tokenized_list = [
        tokenizer(text, max_length=tokenizer.model_max_length, truncation=True) for text in strings
    ]
    input_ids = labels = [np.array(tokenized.input_ids) for tokenized in tokenized_list]
    input_ids_lens = labels_lens = [len(tokenized.input_ids) for tokenized in tokenized_list]

    return dict(input_ids=input_ids, labels=labels, input_ids_lens=input_ids_lens, labels_lens=labels_lens)

def preprocess(sources: Sequence[str], targets: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:
    """预处理数据（分词、生成标签）"""
    examples = [s + t for s, t in zip(sources, targets)]
    examples_tokenized, sources_tokenized = [_tokenize_fn(strings, tokenizer) for strings in (examples, sources)]

    input_ids = examples_tokenized["input_ids"]
    labels = copy.deepcopy(input_ids)

    for label, source_len in zip(labels, sources_tokenized["input_ids_lens"]):
        label[:source_len] = IGNORE_INDEX

    return dict(input_ids=input_ids, labels=labels)

@dataclass
class DataCollatorForSupervisedDataset:
    """数据收集器"""
    tokenizer: transformers.PreTrainedTokenizer

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        input_ids, labels = tuple([instance[key] for instance in instances] for key in ("input_ids", "labels"))
        input_ids = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(x) for x in input_ids], batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        labels = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(x) for x in labels], batch_first=True, padding_value=IGNORE_INDEX
        )
        return dict(input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id))

# 数据分词函数
def train_tokenize_function(examples, tokenizer):
    """处理中文医疗数据格式"""
    if all(field in examples for field in ["question", "reasoning (reasoning_content)", "response (content)"]):
        sources = [build_instruction_prompt(f"{q}\n{r}") for q, r in zip(examples["question"], examples["reasoning (reasoning_content)"])]
        targets = [f"{resp}\n{EOT_TOKEN}" for resp in examples["response (content)"]]

        return preprocess(sources, targets, tokenizer)
    
    missing_fields = [f for f in ["question", "reasoning (reasoning_content)", "response (content)"] if f not in examples]
    raise ValueError(f"数据样本缺少必要字段: {', '.join(missing_fields)}。")

# 构建模型
def build_model(model_args, training_args, checkpoint_dir):
    compute_dtype = torch.bfloat16 if training_args.bf16 else torch.float16
    quantization_config = (
        BitsAndBytesConfig(load_in_4bit=True, load_in_8bit=False) if model_args.use_lora and model_args.bits < 16 else None
    )
```

我们本周一个部署，三个训练，两个方向数据集整理 

1、部署[Phi4-microsoft/Phi-4-multimodal-instruct · Hugging Face](https://huggingface.co/microsoft/Phi-4-multimodal-instruct "https://huggingface.co/microsoft/phi-4-multimodal-instruct") 周一内完成 ，在azure上vm主机[guanzongphi4](https://portal.azure.com/#@safephone.cn/resource/subscriptions/93c98a12-1616-44ed-8863-801aa4bb6028/resourceGroups/GPUxunlian/providers/Microsoft.Compute/virtualMachines/guanzongphi4 "https://portal.azure.com/#@safephone.cn/resource/subscriptions/93c98a12-1616-44ed-8863-801aa4bb6028/resourcegroups/gpuxunlian/providers/microsoft.compute/virtualmachines/guanzongphi4")

2、模型训练

2.1训练并部署医疗phi4 [yanshiyiliao](https://portal.azure.com/#@safephone.cn/resource/subscriptions/93c98a12-1616-44ed-8863-801aa4bb6028/resourceGroups/GPUxunlian/providers/Microsoft.Compute/virtualMachines/yanshiyiliao "https://portal.azure.com/#@safephone.cn/resource/subscriptions/93c98a12-1616-44ed-8863-801aa4bb6028/resourcegroups/gpuxunlian/providers/microsoft.compute/virtualmachines/yanshiyiliao") 周一完成，

2.2训练并部署sft+rl训练的PHI4 [yanshi](https://portal.azure.com/#@safephone.cn/resource/subscriptions/93c98a12-1616-44ed-8863-801aa4bb6028/resourceGroups/GPUxunlian/providers/Microsoft.Compute/virtualMachines/yanshi "https://portal.azure.com/#@safephone.cn/resource/subscriptions/93c98a12-1616-44ed-8863-801aa4bb6028/resourcegroups/gpuxunlian/providers/microsoft.compute/virtualmachines/yanshi")周一完成，  

2.3完成训练phi3.5 moe+医疗。  周二、三完成

3、处理数据集、藏和台方向。3k-10k左右。周三、四完成。

microsoft/Phi-4-multimodal-instruct · Hugging Face

We’re on a journey to advance and democratize artificial intelligence through open source and open science. 2.2和2.3我做，2.2我已经要快完成了。 上面完了，就解决ep通讯，然后完成风筝的训练 [xunlian](https://portal.azure.com/#@safephone.cn/resource/subscriptions/93c98a12-1616-44ed-8863-801aa4bb6028/resourceGroups/GPUxunlian/providers/Microsoft.Compute/virtualMachines/xunlian "https://portal.azure.com/#@safephone.cn/resource/subscriptions/93c98a12-1616-44ed-8863-801aa4bb6028/resourcegroups/gpuxunlian/providers/microsoft.compute/virtualmachines/xunlian") 是双卡的h100，可以拿去研究ep通讯。

Microsoft Azure