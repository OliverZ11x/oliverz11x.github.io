---
date created: 2025/7/9 16:10
date modified: 2025/7/11 8:58
---

车辆的库存信息，汽车 id，接车日期，指导价格，以及车辆相关数据进行业务层面的快速查询。

主要用到的工具是 Langchain 和 langgraph 框架下的 SqlAgent ，langsmith 等工具

主要是在 vllm 框架下通过 docker 实现模型的高性能部署，在保证高并发量的同时快速进行推理

LoRA，GRPO

机审模型运营专家

1、负责内容质量策略的编制及迭代；

2、通过算法模型、工具等自动化产品运营手段不断优化迭代相关产品；

3、根据行业大模型需求，设计和构建合适的模型，进行实验和测试，并不断优化模型的性能和效果。

职位要求

1、本科及以上学历，有2年以上内容质量/风控/安全相关行业经验优先；

2、业务理解能力强，从业务角度理清需求，推动模型优化；

3、有强的沟通协调能力，作为项目负责人承担产品风险治理的工作，协调推进各方达到项目目标；

4、了解模型技术，参与过模型的训练和优化选代过程，熟练掌握Prompt编写、应用和优化技巧，了解Al Agents原理，对人机辅助有兴趣有行动。

---

## 项目热点

## AI Agent

### Langsmith

LangSmith 是 LangChain 推出的 LLM 应用调试、监控和评测平台，它的核心作用有四个：

第一，==日志追踪==，能详细记录整个应用链路中的输入、输出、检索过程，方便调试；

第二，==Prompt 调试==，可以快速优化 Prompt，找到最佳提示词；

第三，==应用评测==，支持自动化评测 LLM 输出质量，也可以结合人工评审；

第四，==版本管理==，能追踪不同 Prompt 或应用版本的效果变化，方便版本迭代和回溯。

我在项目中主要用它来调试复杂 Agent 的流程，比如 SQL Agent 和 RAG 的整合，能很直观地发现错误并优化生成效果。

### 提示词

包括**提示模板设计、Few-shot 示例、指令清晰化、角色设定**等技巧；

第一，先==明确任务目标==和范围，确保模型聚焦核心问题；

第二，==指令尽量具体、清晰，减少模糊空间==，尤其是==明确输出格式==；

第三，对于复杂任务，我会采用分步推理的提示策略，提升正确率；

第四，适当加入==高质量 Few-shot 示例==，增强模型学习能力；

第五，提示词优化是个迭代过程，我会用 LangSmith 等工具持续评测、打标签，不断优化。

==使用 xml 标签==

预填充模型的回复

### RAG

向量库

Fassi 向量库，GraphRAG

#### 文档切分粒度不好把控，既担心噪声太多又担心语义信息丢失？

我会用==滑动窗口切分==，保证相邻段落有重叠，避免信息割裂；

结合语义感知切分，比如按段落、标题、标点，或==用 LLM 做预处理==，提升切分自然度；

在检索后，我会==加 Reranker 模型==对段落重新排序，进一步减少噪声，提升回答效果。

## 模型微调

微调方式，不同方式有何区别

**第一步是预训练**，
这个阶段的核心目标是让模型具备通用的语言能力和知识。
做法是在大规模文本数据上，训练模型去预测下一个词，
也就是常说的自回归语言建模，使用的主要技术是 Transformer 和交叉熵损失。
通过这个过程，模型会掌握语法、常识以及基本的推理能力。

---

**第二步是指令微调**，
目的是让模型学会听懂指令、执行具体任务。
方法是用专门的指令数据集，比如问答、翻译、摘要，让模型在监督学习下继续训练。
这样可以让模型从单纯的语言生成，转变为可以按指令输出有用的结果。

---

**第三步是奖励模型训练**，
这一步是为了评估模型输出的质量。
通常做法是让人类评审模型生成的多个回复，然后训练一个奖励模型，
让它能够自动打分，判断哪个回答更符合人类偏好。

---

**最后是强化学习对齐，也就是 RLHF**。
这一步的核心是用奖励模型给出的分数作为奖励信号，
用强化学习算法，比如 ==PPO==，去微调语言模型，让模型倾向生成高分、有用、符合人类价值的回答。
整个过程是生成、打分、优化，循环迭代，最终让模型在能力、可控性和安全性之间达到平衡。

### 对齐与强化学习微调

**第一步，是训练奖励模型。**
这个阶段，我们会让人类评审模型生成的不同回答，然后给出优劣判断，训练出一个奖励模型，
这个奖励模型的作用，就是自动给模型生成的回答打分，模拟人类偏好。

- 自动评分：比如让生成的代码自动编译并运行特定用例测试，通过测试覆盖率、执行速度等指标给出 reward；
- 人类评分：请程序员给一些模型生成的代码打分，然后训练出一个打分模型；
- 混合方式：先用人类打分数据训练一个基础的 reward model，然后再加一些自动化脚本辅助。

**第二步，是生成回答并打分。**
我们用当前的语言模型去生成大量回答，然后用奖励模型给这些回答打分，得出好坏的“奖励信号”。

**第三步，是强化学习优化。**
这一步我们用强化学习的方法，通常是 PPO 算法，把奖励信号反馈给语言模型，调整模型参数。
目标就是让模型更倾向生成高分、符合人类偏好的回答，同时避免更新太大导致模型崩坏。

PPO 在这其中扮演的角色非常适合，因为它通过“近端优化”的思路，一次性更新大量参数，却避免走得太远。此外，语言模型往往很庞大，包含数亿乃至千亿参数，==PPO 强调小步前进+裁剪更新==，就显得十分友好。

GRPO的原理是在传统 PPO 算法的基础上，==加入了一个奖励泛化正则项==，这个正则项会鼓励模型不仅在训练数据上获得高奖励，同时也能在新的未见数据上保持高质量输出，从而提升模型的泛化能力。

GRPO相当于“PPO的语言模型专用版”，适合奖励稀疏、格式严格的生成任务。

	1. **答案相关性奖励**（确保回答包含关键信息）

	2. **严格格式奖励**（XML 结构化输出）

	3. **宽松格式奖励**（允许小偏差）

	4. **XML 标签计数奖励**（检查 XML 结构完整性）

### 微调数据集，数据集制作的时候有什么需要注意的

收集原始数据：首先，您需要收集与目标任务相关的原始数据。这可以是==对话数据、分类数据、生成任务数据==等，具体取决于您的任务类型。确保数据集具有代表性和多样性，以提高模型的泛化能力。

标注数据：对原始数据进行标注，为每个样本提供正确的标签或目标输出。标签的类型取决于您的任务，可以是分类标签、生成文本、对话回复等。确保标注的准确性和一致性。

划分数据集：将标注数据划分为训练集、验证集和测试集。通常，大部分数据用于训练，一小部分用于验证模型的性能和调整超参数，最后一部分用于最终评估模型的泛化能力。

数据预处理：根据任务的要求，对数据进行预处理。这可能包括文本清洗、分词、去除停用词、词干化等处理步骤。确保数据格式和特征表示适合模型的输入要求。

格式转换：将数据转换为适合模型训练的格式。这可能涉及将数据转换为文本文件、JSON格式或其他适合模型输入的格式。

模型微调：使用转换后的数据对基座模型进行微调。

## Transformer

### 多头注意力

![[IMG-2025-07-10-10-13-20.png]]

![](docs/01attachment/docs/Work/求职知识储备/面试算法/IMG-2025-03-26-11-58-36-5.png)

输入的 token 先经过位置编码保留顺序信息，输入经过线性变换生成 Q K V ，再将其乘以注意力头的数量，sequence length，注意力头的尺寸，变成四维矩阵。之后，对 $QK^T$ 进行点积，再除以根号 D，用于回到 (0,1) 的正态分布。再加上 mask 矩阵。最后通过 softmax 进行归一化，得到最后的注意力分数，用 V 矩阵和注意力分数进行计算

- **输入嵌入（Token Embedding + Positional Encoding）**：
	
	- 把输入 token 转换成向量（embedding），
	- 加上位置编码，保留顺序信息。
		
- **生成 Q、K、V**：
	
	- 输入经过线性变换，生成**Query（Q）、Key（K）、Value（V）**矩阵；
	- 通常 Q、K、V 的维度是**[batch size, sequence length, hidden size]**。
		
- **多头注意力机制（Multi-Head Attention）**：
	
	- 把 Q、K、V 按**注意力头数**分成多份（例如 8个头、16个头），形成四维张量：
		
		- [batch size, num_heads, sequence length, head_dim]；
			
	- 这样每个头能专注于不同的信息子空间。
		
- **计算注意力得分**：
	
	- 计算点积：Q⋅KTQ \cdot K^TQ⋅KT；
		
	- 除以 D\sqrt{D}D​（通常 D 是 head_dim），为了防止梯度消失或爆炸（归一化作用）；
		
	- 可加上 mask（比如因果 mask，防止未来信息泄露）。
		
- **Softmax归一化**：
	
	- 对点积结果做 softmax，得到注意力权重矩阵。
		
- **加权求和**：
	
	- 用注意力权重矩阵和 V 相乘，得到加权和，形成新的特征表示。
		
- **拼接与线性变换**：
	
	- 把所有头的输出拼接起来，再通过一个线性层，生成最终输出。
- 再用前馈网络做特征变换，过程中还会加入残差连接和 Layer Normalization，保证训练稳定。

### Decoder 的输入

==训练阶段==Decoder 的输入是：

- **Encoder的输出（上下文向量）**
- **目标序列的“已知”前缀**（通常是**目标句子的前 N-1 个 token**）

==推理阶段==，没有目标句子可用，只能**靠模型自己生成**。

Decoder 输入是：

- Encoder的输出（上下文向量）
- 当前已经生成的 token 序列（初始输入通常是 `<BOS>` 或 `<START>` 标记）

**是否了解 transfermer呐？请陈述一下自注意力机制**

注意力机制的本质是通过计算输入序列中各个位置之间的相似度，来决定当前这个位置应该“关注”哪些其他位置的信息，并赋予不同的权重。输入序列会被映射成三个向量：q，k，v。模型用 q 和所有位置的 k 计算相似度，根据相似度对对应的 v 加权求和，从而决定当前这个词该关注哪些信息。这种机制让模型可以动态聚焦于对当前词最有用的上下文。

**请解释transformer结构中多头自注意力的作用**

多头注意力就是把 Q、K、V 分成多个子空间，让每个头独立计算注意力，捕捉不同类型的关联信息，比如短程依赖、长程依赖、语义联系等。多个注意力头可以从多个角度理解句子，再把结果融合在一起，让模型更全面地理解输入。

**为什么它对大模型至关重要**

因为大模型参数庞大，需要有强大的信息建模能力来发挥作用。多头自注意力机制可以让每个位置同时关注全局多个信息片段，不仅高效，还能避免信息遗失，是大模型能够理解复杂结构、处理长文本的关键所在。没有这个机制，大模型就难以充分利用其参数容量和计算资源。

**如何用主动学习策略减少金融垂直模型SFT阶段的数据标注成本？**

主动学习策略具体做法是先用少量数据训练模型，然后让模型找出它最不确定的样本，再重点标这些，再用新标的数据继续训练。

**多头注意力这种机制会给大模型造成什么样的缺陷？**

多头注意力机制会显著增加计算资源消耗，且部分注意力头可能冗余，导致效率降低和训练复杂度提升。

**在金融客服场景中，若用户问题涉及专业术语（如LRP利率转换)如何优化检索模块的召回策略？**

先做一个专业术语和同义词的词库、把用户的问题和知识库里的内容都统一处理一下，然后用金融领域的预训练模型把文字转成向量，结合传统的关键词匹配，提升对金融术语的理解和覆盖，从而优化召回效果。

## 位置编码

### 绝对位置编码

这是最早的 Transformer 中使用的位置编码方式，代表性方案是==正弦-余弦位置编码==，也包括可训练的绝对位置向量。

简单，直观

### RoPE 的基本原理

用**旋转矩阵**方式==将 Token 的位置信息编码到 Query 和 Key 中==。把**位置编码嵌入到 Attention 机制内部**，不直接加到词向量上；

| 情景     | 绝对位置编码            | RoPE              |
| ------ | ----------------- | ----------------- |
| 句子开头位置 | 显式告诉“这是第1个 token” | 不关注绝对位置，只关注“相对关系” |
| 长句子推理  | 位置超长会失效           | 理论上可无限长           |
| 记忆相对位置 | 很弱                | 很强                |

### RoPE 缩放的原理

在推理时，对 RoPE 的“角度”进行缩放，==让位置变化“变慢”==，等价于拉伸坐标轴。

YaRN 是目前 RoPE Scaling 中效果非常好的方法之一。

1. **分段线性缩放（Multi-segment Linear Scaling）**：
	- 在不同的长度区间，使用不同的缩放比例；
	- 短上下文几乎不变，长上下文才大幅缩放；
2. **平滑过渡**：
	- 保证短文本和长文本之间的过渡自然，不会出现突变，避免模型突发性能下降；
3. **无需微调**：
	- 在现有权重下即可直接扩展到 131K 甚至更高的上下文长度；

## LoRA 的整体架构与工作机制

### 1️⃣ 原始大模型的瓶颈：

Transformer 中，很多核心矩阵（如 Attention 中的 Q、K、V、Output 投影矩阵，MLP 层的线性层）都是**大规模权重矩阵**，参数量极大：

$$
y = W x
$$

其中 $W \in \mathbb{R}^{d_{out} \times d_{in}}$，参数量是 $d_{out} \times d_{in}$。

---

### 2️⃣ LoRA 的设计：低秩分解

LoRA 的关键操作：

把大矩阵的**权重更新**部分，拆成两个小矩阵乘积：

$$
\Delta W = B \cdot A
$$

其中：

* $A \in \mathbb{R}^{r \times d_{in}}$；
* $B \in \mathbb{R}^{d_{out} \times r}$；
* $r$ 是**低秩维度**（远小于 $d_{in}, d_{out}$，比如 $r=4, 8, 16$）。

==整体公式==：

$$
y = W x + \alpha \cdot B \cdot A \cdot x
$$

其中：

* $W$：冻结，不训练；
* 只训练 $A, B$ 两个小矩阵；
* $\alpha$：缩放因子（调节增量幅度）。

---

### 3️⃣ LoRA 在 Transformer 中的应用：

主要插入在**Attention模块** 和 **MLP模块**中的线性层：

* Q、K、V、O 投影矩阵；
* FFN 层中的 W1、W2。

结构示意：

```python
原始输出 = 冻结的权重层输出 + LoRA 小矩阵输出
```

### LORA 微调相比于微调适配器或前缀微调有什么优势？

Prefix Tuning 就像给模型戴了一顶“帽子”，这顶帽子由一小段“隐形话语”组成，指引模型如何理解和生成内容，而模型本身没有被动手改动。

### 什么是Kv cache技术，它具体是如何实现的？

KV Cache 技术就是在生成过程中，缓存每一步计算出的 Key 和 Value，避免重复计算历史上下文的注意力，极大提升 Transformer 自回归推理效率。

### 简述一下FlashAttention的原理

## 大语言模型分类

| 类别            | 特征                  | 代表模型                                    |
| ------------- | ------------------- | --------------------------------------- |
| **预训练语言理解模型** | 通常用于特征提取、文本理解、分类等   | BERT、RoBERTa                            |
| **语言生成模型**    | 用于文本生成、对话、写作、推理等    | GPT 系列、LLaMA、Qwen3                      |
| **指令微调模型**    | 通过指令微调优化模型对人类指令的响应  | ChatGPT、Qwen3-Instruct、Mistral-Instruct |
| **多模态大模型**    | 支持文字、图片、音频、视频等多模态输入 | Gemini、GPT-4V、MiniGPT-4                 |
| **代码模型**      | 专注于代码生成与理解          | Code Llama、StarCoder、WizardCoder        |

- Encoder-only，比如 BERT，主要用于文本理解、分类、实体识别；
	
- Decoder-only，比如 GPT 系列、LLaMA、Qwen3，主要做文本生成和推理；
	
- Encoder-Decoder，比如 T5、BART，适合翻译、摘要、问答等任务。
